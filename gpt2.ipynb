{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "device = 'cuda:0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/data2/cache\"\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir) # 124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "torch.Size([50257, 768])\n",
      "transformer.wpe.weight\n",
      "torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.0.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.0.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.0.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.1.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.1.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.1.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.1.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.2.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.2.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.2.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.2.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.3.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.3.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.3.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.3.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.4.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.4.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.4.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.4.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.5.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.5.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.5.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.5.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.6.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.6.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.6.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.6.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.7.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.7.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.7.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.7.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.8.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.8.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.8.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.8.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.9.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.9.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.9.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.9.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.10.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.10.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.10.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.10.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.11.ln_1.weight\n",
      "torch.Size([768])\n",
      "transformer.h.11.ln_1.bias\n",
      "torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.h.11.ln_2.weight\n",
      "torch.Size([768])\n",
      "transformer.h.11.ln_2.bias\n",
      "torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "transformer.ln_f.weight\n",
      "torch.Size([768])\n",
      "transformer.ln_f.bias\n",
      "torch.Size([768])\n",
      "lm_head.weight\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "state_dict = model_hf.state_dict()\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    print(key)\n",
    "    print(value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello, how are you? What you\\'re doing is something I really feel like I would never have been able to handle at this time.\"\\n\\nThe boy said, \"I\\'m trying desperately not to sound like I\\'m doing anything strange… I'}, {'generated_text': \"Hello, how are you? So, you won't be the only one that hasn't heard about this. I mean, maybe you wouldn't know, but you're not alone. It seems like a great opportunity. But you know, it's\"}, {'generated_text': \"Hello, how are you? I'm sorry about having forgotten to tell a friend. [pause] Okay, nice and long but the fact seems to go some way to keeping them out of your system. So I'm going to assume we've got\"}, {'generated_text': \"Hello, how are you? I just got my son in from college with $80 million he was already paying… He'll be my kid, too. Please don't get caught, I want to take him to Canada. (The boy looks)\"}, {'generated_text': 'Hello, how are you?\\n\\n\"How are you?\"\\n\\n\"How are you? Your mouth is really large. It\\'s so hard and wide. My mouth feels so good, it\\'s so small, but it\\'s nice and smooth'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like you! So much for that, now go look at this beautiful picture you received of me, of a huge, beautiful, beautiful girl who came into my bedroom.\" (In case you missed her, it's a cute image of a naked\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like you! I like you! I like you!\", he said.\n",
      "\n",
      "In July 2015, the police raided her house in the north eastern suburb of Malabar, after the mother of one had been found guilty of murder for murdering a\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like you!\n",
      "\n",
      "I've not seen the video.\"\n",
      "\n",
      "Bryant responded with, \"Really? Why don't you have a look?\"\n",
      "\n",
      "The man laughed and then took out his phone to see what his wife was doing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like you! Good Morning! [kiss] I'll show you my new look, but we won't be sitting around long together, after all. You might as well be talking to me before I put your face back onto my lips. [\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like you! I understand.\n",
      "\n",
      "And you are too polite.\n",
      "\n",
      "I like you!\n",
      "\n",
      "Yesss!\n",
      "\n",
      "And my hair, not long and sleek! but short and sleek!\n",
      "\n",
      "You know I'm wearing\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = generator(\"I like you!\", max_length=50, num_return_sequences=5)\n",
    "for t in text:\n",
    "    print(t['generated_text'])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-25 17:45:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2024-12-25 17:45:47 (48.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /data2/hanchi/miniconda3/envs/trl/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /data2/hanchi/miniconda3/envs/trl/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data2/hanchi/miniconda3/envs/trl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data2/hanchi/miniconda3/envs/trl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data2/hanchi/miniconda3/envs/trl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data2/hanchi/miniconda3/envs/trl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338025\n",
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokens = enc.encode(text)\n",
    "print(len(tokens))\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "buf = torch.tensor(tokens[:24+1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
